---
title: "End-to-End Fitness Analytics Platform (LLM-Ready)"
author: "Gabriel Ferreira"
date: "2026-02-12"
format:
  html:
    page-layout: full
    code-fold: true
    code-summary: "Show Code"
    toc: true
    toc-depth: 1
    toc-title: "Project Index"
    anchor-sections: false
engine: jupytern
jupyter: python3
---

# Introduction

This project implements a complete end-to-end data engineering pipeline for strength training and nutrition data.

The objective was to design a production-style analytical data system capable of:

- Auditing and validating raw operational data
- Modeling relationally in PostgreSQL
- Executing idempotent ETL pipelines
- Producing deterministic weekly aggregations
- Generating a canonical JSON contract for LLM consumption
- Running in AWS under a controlled, low-cost architecture

---

# Architecture Overview

```{mermaid}
graph TD
  A[Raw CSV: Hevy + Diet] --> B[Cleaning & Validation: Silver]
  B --> C[PostgreSQL: Normalized Schema]
  C --> D[Gold Weekly Views]
  D --> E[Canonical JSON Contract]
  E --> F[Amazon Bedrock: LLM Analysis]
```


The system was designed for deterministic weekly reproducibility.

---

# Data Sources

## Workout Data (Hevy)

- Format: CSV export
- Granularity: 1 row = 1 set
- Historical coverage: 2023–2026
- Total rows: 9542
- Total unique exercises: 118

## Diet Data

- Source: SQLite (personal databse) → CSV export
- Granularity: 1 row per day
- Includes:
  - Macros
  - Calories
  - Cardio volume
  - Bodyweight
  - Cycle classification

---

# Stage 1: Raw Workout Data

Before modeling or automation, a full exploratory and quality audit was conducted.

```{python}
#| include: false
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
```

##### Iinitial Load:
```{python}
df = pd.read_csv('hevy_workouts_raw.csv')
df.shape
```

##### Sample:
```{python}
df.head(10)
```

##### Types:
```{python}
df.dtypes
```

##### Columns:
```{python}
df.columns
```

There is no need to capture the training start and end times along with the date; therefore, we must split them into a column for the training date (training_date) and another for the training duration (derived from start_time and end_time).

### EDA - Exploratory Data Analysis

##### Checking Missing Values:
```{python}
valores_ausentes = df.isnull().sum().sort_values(ascending = False)
print(valores_ausentes)
```

- superset_id: Never used; consistently contains null values.
- rpe: Same status as superset_id (all values missing).
- distance_km: Only applicable for treadmill workouts; usually null as I track by time.
- duration_seconds: Cardio duration; only present in cardio exercises.
- exercise_notes: Exercise-specific comments; optional/not always required.
- description: Training session description added upon completion; also optional.
- weight_kg: Null for bodyweight exercises and cardio (no load).
- reps: To be analyzed.

##### weight_kg column missing values where 'weight_kg' is NaN and exercise type is not cardio:
```{python}
exercicios_excluidos = ['Treadmill', 'Stair Machine (Floors)', 'Stair Machine (Steps)', 'Stair Machine','Spinning', 'Walking', 'Air Bike']

linhas_ausentes_carga = df[
    df['weight_kg'].isna() &
    ~df['exercise_title'].isin(exercicios_excluidos)
]
linhas_ausentes_carga
```

#### Visualizing missing value distribution:
```{python}
linhas_ausentes_carga = df[
    df['weight_kg'].isna() &
    ~df['exercise_title'].isin(exercicios_excluidos)
]
linhas_ausentes_carga['exercise_title'].value_counts()
```

Missing values for Back Extension (Hyperextension), Pull Up, Knee Raise Parallel Bars, Leg Raise Parallel Bars, Bench Dip, Decline Crunch, Chin Up, Chest Dip (Assisted), Romanian Chair Sit Ups, Decline Crunch (Weighted), Squat (Bodyweight), Push Up, and Plank are expected as these are bodyweight exercises. However, Bicep Curl (Machine) should not have missing values, and Squat (Smith Machine) was likely a zero-load warm-up, but we will investigate further.

#### Filtering these two exercises with null values in weight_kg:
```{python}
exercicios_interesse = [
    'Squat (Smith Machine)',
    'Bicep Curl (Machine)'
]

df[
    df['weight_kg'].isna() &
    df['exercise_title'].isin(exercicios_interesse)
]
```

We confirmed the Smith Machine was a warm-up, but there is likely an error in the bicep exercise, as it is a set to failure and the set_index is 2, meaning it is the third set of this exercise.

#### Analyzing exactly this exercise on this specific date:
```{python}
df[
    (df['title'] == 'Costas + Biceps + Lombar - Treino 2') &
    (df['exercise_title'] == 'Bicep Curl (Machine)') &
    (df['start_time'].str.contains('14 May 2024'))
]
```

After analysis, we confirmed a manual entry error, resulting in the missing value; the correct value is 54.0 kg.

##### Imputing the correct value for this set:
```{python}
df.loc[6644, 'weight_kg'] = 54.0
```

##### reps column missing values where 'reps' is NaN and exercise type is not cardio:
```{python}
exercicios_excluidos = ['Treadmill', 'Stair Machine (Floors)', 'Stair Machine (Steps)', 'Stair Machine','Spinning', 'Walking', 'Air Bike']

linhas_ausentes_repeticoes = df[
    df['reps'].isna() &
    ~df['exercise_title'].isin(exercicios_excluidos)
]

linhas_ausentes_repeticoes
```

Plank is also an exercise where the absence of repetitions is expected and acceptable

### Outlier Detectition

##### Function to detect outliers using the IQR (Interquartile Range) method:
```{python}
def detectar_outliers(df, coluna):
    Q1 = df[coluna].quantile(0.25)
    Q3 = df[coluna].quantile(0.75)
    IQR = Q3 - Q1
    limite_inferior = Q1 - 1.5 * IQR
    limite_superior = Q3 + 1.5 * IQR
    
    outliers = df[(df[coluna] < limite_inferior) | (df[coluna] > limite_superior)]
    return outliers
```

##### Detecting and displaying total outliers per column:
```{python}
colunas = ['weight_kg', 'reps']
for coluna in colunas:
    outliers = detectar_outliers(df, coluna)
    print(f"Total de outliers em '{coluna}': {len(outliers)}")
```

##### Detect and display outliers for the 'weight_kg' column:
```{python}
outliers_carga_kg = detectar_outliers(df, 'weight_kg')

if not outliers_carga_kg.empty:
    print("Outliers em 'weight_kg (119) ':")
    print(outliers_carga_kg[['exercise_title', 'start_time', 'weight_kg', 'reps']])
```

##### Groups and counts how many outliers there are by exercise title:
```{python}
# 1. Detecta todos os outliers novamente
outliers_original = detectar_outliers(df, 'weight_kg')
# 2. Agrupa e conta quantos outliers existem por título de exercício
contagem_outliers = outliers_original['exercise_title'].value_counts()
print("Distribuição de Outliers por Exercício:")
print(contagem_outliers)
```

Legpress is an exercise where the absolute weight is significantly higher compared to the other exercises but they are not necessarily outliers

##### Leg Press (Machine) visualization:
```{python}
exercise_name = "Leg Press (Machine)"
df_ex = df[df["exercise_title"] == exercise_name]
fig = px.box(df_ex,y="weight_kg",points="all",hover_data=["reps","set_index","set_type","rpe","start_time"],title=f"Weight Distribuiton — {exercise_name}")
fig.update_layout(yaxis_title="Weight",xaxis_title="",showlegend=False,height=600,width=900,title_x = 0.5)
fig.show()
```

Correct values. Since my training is based on progressive overload, it makes sense that values close to 280 kg have few records, as they represent the most recent loads lifted.

##### Detect and display outliers for the 'reps' column:
```{python}
outliers_repeticoes = detectar_outliers(df, 'reps')

if not outliers_repeticoes.empty:
    print("Outliers em 'repeticoes (209) ':")
    print(outliers_repeticoes[['exercise_title','weight_kg', 'reps']])
```

##### Group and count outliers per exercise title:
```{python}
outliers_original = detectar_outliers(df, 'reps')

# 2. Agrupa e conta quantos outliers existem por título de exercício
contagem_outliers = outliers_original['exercise_title'].value_counts()

print("Distribuição de Outliers por Exercício:")
print(contagem_outliers)
```

##### Standing Calf Raise (Machine):
```{python}
exercise_name = "Standing Calf Raise (Machine)"
df_ex = df[df["exercise_title"] == exercise_name]
fig = px.box(df_ex,y="reps",points="all",hover_data=["reps","weight_kg","set_index","set_type","rpe","start_time"],title=f"Reps Distribuiton — {exercise_name}")
fig.update_layout(yaxis_title="Reps",xaxis_title="",showlegend=False,height=600,width=900,title_x = 0.5)
fig.show()
```

We evaluated all exercises flagged as outliers and the majority (as seen in the example above) were warm-up sets where no load or a very low load was used; therefore, they do not qualify as incorrect data. The only exception was the Preacher Curl (Machine), detailed below, which indeed contained a manual entry error in the number of repetitions and was handled separately.

##### Preacher Curl (Machine):
```{python}
exercise_name = "Preacher Curl (Machine)"
df_ex = df[df["exercise_title"] == exercise_name]
fig = px.box(df_ex,y="reps",points="all",hover_data=["reps","weight_kg","set_index","set_type","rpe","start_time"],title=f"Reps Distribuiton — {exercise_name}")
fig.update_layout(yaxis_title="Reps",xaxis_title="",showlegend=False,height=600,width=900,title_x = 0.5)
fig.show()
```

Here we have an incorrect value: where 111 is recorded, it should actually be 11 repetitions

##### Analyzing this specific exercise instance:
```{python}
df[
    (df['exercise_title'] == 'Preacher Curl (Machine)') &
    (df['start_time'].str.contains('23 Jan 2024'))
]
```

##### Imputing the correct value for this set:
```{python}
df.loc[8605, 'reps'] = 11.0
```

##### Revalidating the corrected set:
```{python}
df[
    (df['exercise_title'] == 'Preacher Curl (Machine)') &
    (df['start_time'].str.contains('23 Jan 2024'))
]
```

### Validations

##### set_index:
```{python}
# Ordenando para garantir consistência
df_sorted = df.sort_values(
    by=["start_time", "exercise_title", "set_index"]
)

# Agrupando por treino + exercício
set_index_check = (
    df_sorted
    .groupby(["start_time", "exercise_title"])["set_index"]
    .apply(list)
    .reset_index(name="set_index_list")
)

# Funções de validação
def starts_with_zero(seq):
    return min(seq) == 0

def has_gaps(seq):
    return sorted(seq) != list(range(min(seq), max(seq) + 1))

# Aplicando validações
set_index_check["starts_with_zero"] = set_index_check["set_index_list"].apply(starts_with_zero)
set_index_check["has_gaps"] = set_index_check["set_index_list"].apply(has_gaps)

# Casos problemáticos
problemas_set_index = set_index_check[
    (~set_index_check["starts_with_zero"]) |
    (set_index_check["has_gaps"])
]

print(f"Total de exercícios analisados: {len(set_index_check)}")
print(f"Casos com problemas de set_index: {len(problemas_set_index)}")

problemas_set_index.head()
```

##### Workout Duration:
```{python}
# Convertendo para datetime
df["start_time_dt"] = pd.to_datetime(df["start_time"], format="%d %b %Y, %H:%M")
df["end_time_dt"] = pd.to_datetime(df["end_time"], format="%d %b %Y, %H:%M")

# Calculando duração em minutos
df["duracao_treino_min"] = (
    (df["end_time_dt"] - df["start_time_dt"])
    .dt.total_seconds() / 60
)

# Agregando por treino (um treino pode ter várias linhas)
duracao_por_treino = (
    df.groupby(["title", "start_time"])
    .agg(
        duracao_min=("duracao_treino_min", "first")
    )
    .reset_index()
)

# Treinos muito longos (> 4 horas)
treinos_longos = duracao_por_treino[
    duracao_por_treino["duracao_min"] > 240
]

# Treinos muito curtos (< 10 minutos)
treinos_curtos = duracao_por_treino[
    duracao_por_treino["duracao_min"] < 10
]

print(f"Treinos > 4h: {len(treinos_longos)}")
print(f"Treinos < 10min: {len(treinos_curtos)}")

treinos_longos.head(), treinos_curtos.head()
```

##### Naming Consistency:
```{python}

# Total de exercícios únicos
exercicios_unicos = df["exercise_title"].nunique()

# Frequência por exercício
freq_exercicios = df["exercise_title"].value_counts()

# Exercícios que aparecem apenas 1 vez
exercicios_uma_vez = freq_exercicios[freq_exercicios == 1]

print(f"Total de exercícios únicos: {exercicios_unicos}")
print(f"Exercícios que aparecem apenas 1 vez: {len(exercicios_uma_vez)}")

exercicios_uma_vez.head(10)
```

### Analysis

##### Workouts per day:
```{python}
# Treinos por dia
df["data_treino"] = df["start_time_dt"].dt.date

treinos_por_dia = (
    df.groupby("data_treino")["title"]
    .nunique()
    .reset_index(name="qtde_treinos")
)

fig = px.bar(
    treinos_por_dia,
    x="data_treino",
    y="qtde_treinos",
    title="Training Frequency Over Time"
)

fig.update_layout(
    xaxis_title="Data",
    yaxis_title="Workout Count",
    title_x=0.5,
    height=500
)

fig.show()
```

##### Load progression - Exercise: Seated Leg Curl (Machine):
```{python}
exercise_name = "Seated Leg Curl (Machine)"

df_ex = (
    df[df["exercise_title"] == exercise_name]
    .groupby("data_treino")
    .agg(carga_max=("weight_kg", "max"))
    .reset_index()
)

fig = px.line(
    df_ex,
    x="data_treino",
    y="carga_max",
    markers=True,
    title=f"Load progression — {exercise_name}"
)

fig.update_layout(
    xaxis_title="Data",
    yaxis_title="Maximum Weight (kg)",
    title_x=0.5,
    height=500
)

fig.show()
```

##### Load progression - Exercise: Chest Fly (Machine):
```{python}
exercise_name = "Chest Fly (Machine)"

df_ex = (
    df[df["exercise_title"] == exercise_name]
    .groupby("data_treino")
    .agg(carga_max=("weight_kg", "max"))
    .reset_index()
)

fig = px.line(
    df_ex,
    x="data_treino",
    y="carga_max",
    markers=True,
    title=f"Load progression — {exercise_name}"
)

fig.update_layout(
    xaxis_title="Data",
    yaxis_title="Maximum Weight (kg)",
    title_x=0.5,
    height=500
)

fig.show()
```

##### Scatter plot: weight vs. rep:
```{python}
fig = px.scatter(
    df,
    x="weight_kg",
    y="reps",
    color="set_type",
    opacity=0.6,
    title="Weight and Reps Scatter",
    hover_data=["exercise_title", "data_treino"]
)

fig.update_layout(
    xaxis_title="Wight (kg)",
    yaxis_title="Repetitions",
    title_x=0.5,
    height=600
)

fig.show()
```

##### Workout duration histogram:
```{python}
fig = px.histogram(
    duracao_por_treino,
    x="duracao_min",
    nbins=30,
    title="Workout Duration Distribution (min)"
)

fig.update_layout(
    xaxis_title="Duratiom (min)",
    yaxis_title="Train Count",
    title_x=0.5,
    height=500
)

fig.show()
```

##### Most frequent exercises:
```{python}
top_exercicios = (
    df["exercise_title"]
    .value_counts()
    .head(15)
    .reset_index()
)

top_exercicios.columns = ["exercise_title", "frequencia"]

fig = px.bar(
    top_exercicios,
    x="frequencia",
    y="exercise_title",
    orientation="h",
    title="Most Frequent Exercises (Top 15)"
)

fig.update_layout(
    xaxis_title="Number of sets",
    yaxis_title="",
    title_x=0.5,
    height=600,
    yaxis={'categoryorder':'total ascending'}
)

fig.show()
```

##### Set types (distribution):
```{python}
fig = px.pie(
    df,
    names="set_type",
    title="Set Type Distribution"
)

fig.update_layout(
    title_x=0.5,
    height=500
)

fig.show()
```

##### Training volume:
```{python}
# Garantindo a coluna de data
df["data_treino"] = df["start_time_dt"].dt.date

# Cada linha representa uma série → volume = contagem de linhas
volume_series_diario = (
    df.groupby("data_treino")
    .size()
    .reset_index(name="total_series")
)

fig = px.line(
    volume_series_diario,
    x="data_treino",
    y="total_series",
    markers=True,
    title="Trainning Volume per Day (Number of sets)"
)

fig.update_layout(
    xaxis_title="Data",
    yaxis_title="Total Sets",
    title_x=0.5,
    height=500
)

fig.show()
```

##### Set voulme by type:
```{python}
df["data_treino"] = df["start_time_dt"].dt.date

df_efetivas = df[df["set_type"].isin(["normal", "failure"])]

volume_por_tipo = (
    df_efetivas
    .groupby(["data_treino", "set_type"])
    .size()
    .reset_index(name="total_series")
)

fig = px.area(
    volume_por_tipo,
    x="data_treino",
    y="total_series",
    color="set_type",
    title="Total Sets per Type"
)

fig.update_layout(
    xaxis_title="Data",
    yaxis_title="Total Sets",
    title_x=0.5,
    height=500
)

fig.show()
```

##### Final Dataset Export (Silver Layer):
```{python}
df.to_csv("hevy_workouts_clean.csv",index=False,encoding="utf-8")
```

# Stage 2: Raw Diet Data

Before integrating nutrition data into the relational model, a full validation and structural audit was performed on the exported dataset from my personal SQLite database.

The objective of this stage was to:

* Validate structural integrity
* Ensure domain consistency
* Confirm macro–calorie coherence
* Normalize cycle identification
* Produce an immutable clean dataset for ingestion

##### Initial Load:
```{python}
pd.set_option("display.max_rows", 200)
pd.set_option("display.max_colwidth", None)
df = pd.read_csv("diet_daily_raw_export.csv")
df.head()
```

##### Shape:
```{python}
df.shape
```

##### Structural Overview:
```{python}
df.info()
```

Observations:

* No missing values in numeric macro columns
* observacoes is optional (notes field)
* data initially loaded as object (string)
* cicle stored as free-text categorical field

##### Granularity validation:
```{python}
df.duplicated(subset=["data"]).sum()
```

This confirms the dataset respects the intended grain:

1 row = 1 day

##### Date conversion:
```{python}
df["data"] = pd.to_datetime(df["data"], errors="coerce")
df[df["data"].isna()]
```

No invalid dates detected.

This confirms full temporal integrity.

##### Missing Values Audit:
```{python}
df.isna().sum()
```

* All macro and calorie fields fully populated
* observacoes contains null values (expected and acceptable)
* No missing bodyweight records
* No missing cycle classification
* No unexpected structural gaps were found.

#### Domain Validations:
##### Negative Value Check:
```{python}
numeric_cols = [
    "carboidratos_g",
    "proteinas_g",
    "gorduras_g",
    "kcal",
    "cardio_semanal_min",
    "peso_kg"
]

(df[numeric_cols] < 0).sum()
```

All zero.

No invalid negative values exist in:

* Macros
* Calories
* Weekly cardio
* Bodyweight

##### Caloric Consistency Validation:

Calories were validated against macro-derived energy:

* Carbs = 4 kcal/g
* Protein = 4 kcal/g
* Fat = 9 kcal/g
```{python}
df["kcal_estimado"] = (
    df["carboidratos_g"] * 4 +
    df["proteinas_g"] * 4 +
    df["gorduras_g"] * 9
)

df[["kcal", "kcal_estimado"]].head(10)
```

##### Difference distribution:
```{python}
(df["kcal"] - df["kcal_estimado"]).abs().describe()
```

Findings:

* Mean deviation ≈ 0.02 kcal
* Maximum deviation: 20 kcal
* 75% of records show zero difference

#### Cycle Normalization:
The cicle column contains descriptive strings combining:

* Sequential cycle number
* Strategy (Cutting, Bulking, Reverse, Maintenance)
* Date range

##### Unique values::
```{python}
df["cicle"].unique()
```

To normalize this for relational modeling, a deterministic mapping was created:

##### Cycle map:
```{python}
cycle_map = {
    "1Cutting I (01/05/2023 - 17/09/2023)": 1,
    "2Reversa I (18/09/2023 - 29/10/2023)": 2,
    "3Cutting II (30/10/2023 - 23/02/2024)": 3,
    "4Reversa II (24/02/2024 - 22/03/2024)": 4,
    "5Bulking I (23/03/2024 - 19/05/2024)": 5,
    "6Cutting III (20/05/2024 - 28/09/2024)": 6,
    "7Bulking II (28/10/2024 - 01/01/2025)": 7,
    "8Cutting IV (02/01/2025 - 17/04/2025)": 8,
    "9Manutenção I (18/04/2025 - 05/10/2025)": 9,
    "10Cutting V (06/10/2025 - XX/XX/XXXX)": 10
}

df["cycle_id"] = df["cicle"].map(cycle_map)
df[df["cycle_id"].isna()][["data", "cicle"]]
```

Result:

No unmapped values.

This guarantees referential integrity for the future cycles dimension table.

#### Column Standardization:
Columns were renamed to follow the project’s naming conventions:

* English
* Snake case
* Explicit semantic meaning
```{python}
df = df.rename(columns={
    "data": "diet_date",
    "carboidratos_g": "carbs_g",
    "proteinas_g": "protein_g",
    "gorduras_g": "fat_g",
    "kcal": "calories_kcal",
    "cardio_seamanal_min": "cardio_weekly_min",
    "peso_kg": "bodyweight_kg",
    "observacoes": "notes"
})
df = df.drop(columns=["cicle", "kcal_estimado"])
df = df.replace({np.nan: None})
```

##### Final Dataset Export (Silver Layer):
```{python}
df.to_csv(
    "diet_daily_clean.csv",
    index=False,
    encoding="utf-8"
)
```

# Stage 3: Relational Modeling (Amazon RDS)

After validating and cleaning both training and nutrition datasets, the next step was the implementation of a normalized relational model.

This stage was executed in:

> **Amazon RDS — PostgreSQL (Managed Database Service)**

The objective was to:

* Move from flat CSV structure to normalized relational modeling
* Enforce referential integrity
* Guarantee deterministic aggregations
* Enable performant weekly analytics
* Prepare the system for idempotent ETL ingestion

## Infrastructure Context

* **Database engine:** PostgreSQL
* **Deployment:** Amazon RDS
* **Schema:** fitness
* **Connection:** SSL-enabled

RDS was intentionally chosen to:

* Simulate a production-style managed database
* Avoid local-only architecture
* Demonstrate cloud database provisioning
* Keep operational overhead minimal

**Database ER Diagram**

![](fitenssDB-LLM.png)

## Schema Definition

The schema was explicitly defined to isolate all project objects.

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE SCHEMA IF NOT EXISTS fitness;
SET search_path TO fitness;
```
</details>

This ensures:

* Logical separation
* Namespace clarity
* Clean migration management

---

## Core Tables

The modeling follows a normalized structure where:

1 workout → many exercises
1 exercise → many sets
1 day → 1 diet record

Foreign keys enforce consistency across all relationships.

---

### Table: exercises

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE TABLE exercises (
    exercise_id SERIAL PRIMARY KEY,
    exercise_name TEXT NOT NULL UNIQUE,
    exercise_type TEXT,
    is_cardio BOOLEAN,
    created_at TIMESTAMP DEFAULT NOW()
);
```
</details>

#### Design Notes

* `exercise_name` is UNIQUE to prevent duplication during ETL.
* `is_cardio` allows future semantic filtering.
* `exercise_type` reserved for future classification (not hardcoded yet).
* Timestamp ensures traceability.

This table acts as a dimension table for exercises.

### Table: workouts

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE TABLE workouts (
    workout_id SERIAL PRIMARY KEY,
    workout_name TEXT,
    workout_date DATE NOT NULL,
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    duration_minutes NUMERIC(5,2),
    description TEXT,
    source TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);
```
</details>

#### Design Notes

* `workout_date` stored separately for aggregation performance.
* `duration_minutes` precomputed to avoid runtime calculation overhead.
* `source` explicitly tracks ingestion origin (e.g., hevy_csv).
* Not enforcing uniqueness constraint intentionally — allows ingestion flexibility if future multi-source system is introduced.

This table represents the training session grain.

### Table: workout_exercises

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE TABLE workout_exercises (
    workout_exercise_id SERIAL PRIMARY KEY,
    workout_id INT NOT NULL REFERENCES workouts(workout_id) ON DELETE CASCADE,
    exercise_id INT NOT NULL REFERENCES exercises(exercise_id),
    exercise_order INT,
    superset_id INT,
    notes TEXT
);
```
</details>

#### Design Notes

This is a bridge table resolving:

* Many exercises per workout
* Exercise order inside workout
* Superset grouping

`ON DELETE CASCADE` ensures:

* Deleting a workout automatically deletes dependent records.

This avoids orphan data.

### Table: sets
<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE TABLE sets (
    set_id SERIAL PRIMARY KEY,
    workout_exercise_id INT NOT NULL REFERENCES workout_exercises(workout_exercise_id) ON DELETE CASCADE,
    set_index INT NOT NULL,
    set_type TEXT,
    weight_kg NUMERIC(6,2),
    reps INT,
    distance_km NUMERIC(6,3),
    duration_seconds INT,
    rpe NUMERIC(3,1),
    created_at TIMESTAMP DEFAULT NOW()
);
```
</details>

#### Design Notes

This is the most granular fact table.

Grain:

> 1 row = 1 set

Key decisions:

* Numeric precision defined explicitly
* `set_type` not constrained via ENUM to allow ingestion flexibility
* No derived metrics stored (volume, 1RM etc.)
* RPE stored but not enforced

All analytical metrics will be calculated in Gold layer views.

## Performance Indexes

Indexes were created to optimize common aggregation paths.


<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE INDEX idx_workouts_date ON workouts(workout_date);
CREATE INDEX idx_exercises_name ON exercises(exercise_name);
CREATE INDEX idx_sets_type ON sets(set_type);
CREATE INDEX idx_sets_workout_exercise ON sets(workout_exercise_id);
```
</details>

### Index Strategy

* `workout_date` → weekly aggregations
* `exercise_name` → lookup & joins
* `set_type` → filtering warmups/failure sets
* `workout_exercise_id` → join optimization

Indexes were added intentionally after understanding expected query patterns.

### Nutrition Table

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE TABLE diet_daily (
    diet_date DATE PRIMARY KEY,
    carbs_g INT,
    protein_g INT,
    fat_g INT,
    calories_kcal INT,
    cardio_weekly_min INT,
    cycle_id INT
        REFERENCES cycles(cycle_id),
    bodyweight_kg NUMERIC(5,2),
    notes TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```
</details>

#### Design Notes

Grain:

> 1 row = 1 day

Key decisions:

* `diet_date` is PRIMARY KEY → prevents duplicates
* `cycle_id` references a dimension table (`cycles`)
* No macro-derived metrics stored
* `cardio_weekly_min` kept at daily level for consistency with original structure

This table enables:

* Weekly averages
* Cycle segmentation
* Cross-analysis with training

## Referential Integrity Strategy

Foreign keys were enforced in:

* workout_exercises → workouts
* workout_exercises → exercises
* sets → workout_exercises
* diet_daily → cycles

This guarantees:

* No orphan sets
* No unmapped exercises
* No diet record without valid cycle

The model prevents silent corruption.

# Stage 4: ETL Pipeline (Amazon RDS)

After validating and cleaning the raw datasets, the next step was to load the data into a normalized relational schema deployed in **Amazon RDS (PostgreSQL)**.

The ETL pipeline was executed against an Amazon RDS PostgreSQL instance under the `fitness` schema.

## Environment Configuration

Database credentials were managed through environment variables to avoid hardcoding secrets.
```{python}
#| eval: false
import os
from dotenv import load_dotenv

# Carrega o .env da raiz do projeto
load_dotenv()

DB_HOST = os.getenv("DB_HOST")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "postgres")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_SCHEMA = os.getenv("DB_SCHEMA", "fitness")

CSV_PATH = os.getenv("CSV_PATH")
```

This ensures:

* Secure configuration
* Easy migration between local and AWS environments
* No credentials stored in version control

## Database Engine (Amazon RDS Connection)
```{python}
#| eval: false
from sqlalchemy import create_engine
from etl.config.settings import (
    DB_HOST, DB_PORT, DB_NAME,
    DB_USER, DB_PASSWORD, DB_SCHEMA
)

def get_engine():
    if not DB_PASSWORD:
        raise RuntimeError("DB_PASSWORD não foi carregada do .env")

    url = (
        f"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}"
        f"@{DB_HOST}:{DB_PORT}/{DB_NAME}"
    )

    engine = create_engine(
        url,
        connect_args={"options": f"-csearch_path={DB_SCHEMA}"}
    )

    return engine
```

This engine connects directly to:

> Amazon RDS → PostgreSQL → fitness schema

---

## Load Order Strategy

Because of foreign key dependencies, the loading order must respect relational integrity:

1. exercises
2. workouts
3. workout_exercises
4. sets

This guarantees:

* No orphan records
* Valid foreign keys
* Controlled dependency resolution

## Load — Exercises
```{python}
#| eval: false
import pandas as pd
from sqlalchemy import text
from etl.utils.db import get_engine
from etl.config.settings import CSV_PATH


def load_exercises():
    engine = get_engine()

    # 1. Ler CSV (Bronze → memória)
    df = pd.read_csv(CSV_PATH)

    # 2. Extrair exercícios únicos
    exercises = (
        df["exercise_title"]
        .dropna()
        .drop_duplicates()
        .sort_values()
        .to_frame(name="exercise_name")
    )

    # 3. Inferir se é cardio
    cardio_mask = (
        df.groupby("exercise_title")[["distance_km", "duration_seconds"]]
        .apply(lambda x: x.notna().any().any())
    )

    exercises["is_cardio"] = (
        exercises["exercise_name"]
        .map(cardio_mask)
        .fillna(False)
    )

    # 4. Inserção idempotente
    insert_sql = text("""
        INSERT INTO exercises (exercise_name, is_cardio)
        VALUES (:exercise_name, :is_cardio)
        ON CONFLICT (exercise_name) DO NOTHING;
    """)

    with engine.begin() as conn:
        conn.execute(
            insert_sql,
            exercises.to_dict(orient="records")
        )

    print(f"[OK] {len(exercises)} exercises processed.")
```

Key logic:

* Extract unique exercises from CSV
* Infer `is_cardio`
* Insert using `ON CONFLICT DO NOTHING`

Idempotent insert:

<details>
<summary><strong>Show SQL</strong></summary>

```sql
INSERT INTO exercises (exercise_name, is_cardio)
VALUES (:exercise_name, :is_cardio)
ON CONFLICT (exercise_name) DO NOTHING;
```
</details>

This ensures:

* Safe re-runs
* No duplication
* Deterministic state in RDS

---

## Load — Workouts:
```{python}
#| eval: false
import pandas as pd
from sqlalchemy import text
from etl.utils.db import get_engine
from etl.config.settings import CSV_PATH


def load_workouts():
    engine = get_engine()

    # 1. Ler CSV
    df = pd.read_csv(CSV_PATH)

    # 2. Parse datas
    df["start_time"] = pd.to_datetime(df["start_time"])
    df["end_time"] = pd.to_datetime(df["end_time"])

    # 3. Agrupar por sessão de treino
    workouts = (
        df.groupby(["title", "start_time", "end_time"], as_index=False)
        .agg(
            workout_name=("title", "first"),
            description=("description", "first")
        )
    )

    # 4. Campos derivados
    workouts["workout_date"] = workouts["start_time"].dt.date
    workouts["duration_minutes"] = (
        (workouts["end_time"] - workouts["start_time"])
        .dt.total_seconds() / 60
    )
    workouts["source"] = "hevy_csv"

    # 5. SQLs
    select_sql = text("""
        SELECT workout_id
        FROM workouts
        WHERE workout_name = :workout_name
          AND start_time = :start_time
          AND end_time = :end_time
    """)

    insert_sql = text("""
        INSERT INTO workouts (
            workout_name,
            workout_date,
            start_time,
            end_time,
            duration_minutes,
            description,
            source
        )
        VALUES (
            :workout_name,
            :workout_date,
            :start_time,
            :end_time,
            :duration_minutes,
            :description,
            :source
        )
    """)

    inserted = 0

    with engine.begin() as conn:
        for _, row in workouts.iterrows():
            exists = conn.execute(
                select_sql,
                {
                    "workout_name": row["workout_name"],
                    "start_time": row["start_time"],
                    "end_time": row["end_time"],
                }
            ).fetchone()

            if exists:
                continue

            conn.execute(insert_sql, row.to_dict())
            inserted += 1

    print(f"[OK] {inserted} workouts inserted.")
```

Steps:

* Group by session (`title`, `start_time`, `end_time`)
* Derive:

  * workout_date
  * duration_minutes
  * source

Existence check:
<details>
<summary><strong>Show SQL</strong></summary>

```sql
SELECT workout_id
FROM workouts
WHERE workout_name = :workout_name
  AND start_time = :start_time
  AND end_time = :end_time
```
</details>

Only inserts if not found.

This prevents duplicate training sessions on re-execution.

---

## Load — Workout Exercises:
```{python}
#| eval: false
import pandas as pd
from sqlalchemy import text
from etl.utils.db import get_engine
from etl.config.settings import CSV_PATH


def load_workout_exercises():
    engine = get_engine()

    # 1. Ler CSV
    df = pd.read_csv(CSV_PATH)

    # Parse datas
    df["start_time"] = pd.to_datetime(df["start_time"])
    df["end_time"] = pd.to_datetime(df["end_time"])

    # 2. Buscar mapeamento de workouts
    with engine.begin() as conn:
        workouts_map = {
            (row.workout_name, row.start_time, row.end_time): row.workout_id
            for row in conn.execute(text("""
                SELECT workout_id, workout_name, start_time, end_time
                FROM workouts
            """))
        }

        exercises_map = {
            row.exercise_name: row.exercise_id
            for row in conn.execute(text("""
                SELECT exercise_id, exercise_name
                FROM exercises
            """))
        }

    # 3. Gerar estrutura workout_exercises
    records = []
    seen = set()

    for (
        workout_name,
        start_time,
        end_time
    ), group in df.groupby(["title", "start_time", "end_time"]):

        workout_id = workouts_map.get((workout_name, start_time, end_time))
        if not workout_id:
            continue

        exercise_sequence = (
            group["exercise_title"]
            .drop_duplicates()
            .tolist()
        )

        for order, exercise_name in enumerate(exercise_sequence):
            exercise_id = exercises_map.get(exercise_name)
            if not exercise_id:
                continue

            superset_id = (
                group[group["exercise_title"] == exercise_name]["superset_id"]
                .dropna()
                .unique()
            )
            superset_id = int(superset_id[0]) if len(superset_id) > 0 else None

            key = (workout_id, exercise_id, order)
            if key in seen:
                continue
            seen.add(key)

            records.append({
                "workout_id": workout_id,
                "exercise_id": exercise_id,
                "exercise_order": order,
                "superset_id": superset_id,
                "notes": None
            })

    # 4. Inserção idempotente
    insert_sql = text("""
        INSERT INTO workout_exercises (
            workout_id,
            exercise_id,
            exercise_order,
            superset_id,
            notes
        )
        VALUES (
            :workout_id,
            :exercise_id,
            :exercise_order,
            :superset_id,
            :notes
        )
        ON CONFLICT DO NOTHING;
    """)

    with engine.begin() as conn:
        conn.execute(insert_sql, records)

    print(f"[OK] {len(records)} workout_exercises processed.")
```

Responsibilities:

* Map workouts → exercises
* Preserve exercise order
* Preserve superset relationships

Idempotent logic:
<details>
<summary><strong>Show SQL</strong></summary>

```sql
INSERT INTO workout_exercises (...)
VALUES (...)
ON CONFLICT DO NOTHING;
```
</details>

Ensures safe replay.

---

## Load — Sets (Most Granular Layer):
```{python}
#| eval: false
import pandas as pd
from sqlalchemy import text
from etl.utils.db import get_engine
from etl.config.settings import CSV_PATH


def clean_nan(value):
    """
    Converte NaN do pandas para None (NULL no PostgreSQL)
    """
    if pd.isna(value):
        return None
    return value


def load_sets():
    engine = get_engine()

    # 1. Ler CSV
    df = pd.read_csv(CSV_PATH)

    # Parse datas
    df["start_time"] = pd.to_datetime(df["start_time"])
    df["end_time"] = pd.to_datetime(df["end_time"])

    # 2. Mapas auxiliares
    with engine.begin() as conn:
        workouts_map = {
            (row.workout_name, row.start_time, row.end_time): row.workout_id
            for row in conn.execute(text("""
                SELECT workout_id, workout_name, start_time, end_time
                FROM workouts
            """))
        }

        exercises_map = {
            row.exercise_name: row.exercise_id
            for row in conn.execute(text("""
                SELECT exercise_id, exercise_name
                FROM exercises
            """))
        }

        workout_ex_map = {
            (row.workout_id, row.exercise_id, row.exercise_order): row.workout_exercise_id
            for row in conn.execute(text("""
                SELECT workout_exercise_id, workout_id, exercise_id, exercise_order
                FROM workout_exercises
            """))
        }

    inserted = 0

    insert_sql = text("""
        INSERT INTO sets (
            workout_exercise_id,
            set_index,
            set_type,
            weight_kg,
            reps,
            distance_km,
            duration_seconds,
            rpe
        )
        VALUES (
            :workout_exercise_id,
            :set_index,
            :set_type,
            :weight_kg,
            :reps,
            :distance_km,
            :duration_seconds,
            :rpe
        )
    """)

    exists_sql = text("""
        SELECT 1
        FROM sets
        WHERE workout_exercise_id = :workout_exercise_id
          AND set_index = :set_index
    """)

    with engine.begin() as conn:
        for (
            workout_name,
            start_time,
            end_time
        ), group in df.groupby(["title", "start_time", "end_time"]):

            workout_id = workouts_map.get((workout_name, start_time, end_time))
            if not workout_id:
                continue

            # Ordem dos exercícios dentro do treino
            exercise_sequence = (
                group["exercise_title"]
                .drop_duplicates()
                .tolist()
            )

            exercise_order_map = {
                name: idx for idx, name in enumerate(exercise_sequence)
            }

            for _, row in group.iterrows():
                exercise_id = exercises_map.get(row["exercise_title"])
                if not exercise_id:
                    continue

                exercise_order = exercise_order_map.get(row["exercise_title"])
                workout_exercise_id = workout_ex_map.get(
                    (workout_id, exercise_id, exercise_order)
                )

                if not workout_exercise_id:
                    continue

                exists = conn.execute(
                    exists_sql,
                    {
                        "workout_exercise_id": workout_exercise_id,
                        "set_index": int(row["set_index"]),
                    }
                ).fetchone()

                if exists:
                    continue

                conn.execute(
                    insert_sql,
                    {
                        "workout_exercise_id": workout_exercise_id,
                        "set_index": int(row["set_index"]),
                        "set_type": row["set_type"],
                        "weight_kg": clean_nan(row["weight_kg"]),
                        "reps": clean_nan(row["reps"]),
                        "distance_km": clean_nan(row["distance_km"]),
                        "duration_seconds": clean_nan(row["duration_seconds"]),
                        "rpe": clean_nan(row["rpe"]),
                    }
                )

                inserted += 1

    print(f"[OK] {inserted} sets inserted.")
```

This is the most sensitive layer:

* 1 row = 1 set
* Must match workout_exercise_id
* Must avoid duplication

Before insertion:

* Convert NaN → NULL (PostgreSQL compatible)

Existence check:
<details>
<summary><strong>Show SQL</strong></summary>

```sql
SELECT 1
FROM sets
WHERE workout_exercise_id = :workout_exercise_id
  AND set_index = :set_index
```
</details>

If exists → skip
If not → insert

This makes the pipeline:

> Fully idempotent at the lowest grain level

## Pipeline Orchestration:

All steps were orchestrated through a single execution entrypoint:
```{python}
#| eval: false
from etl.load.load_exercises import load_exercises
from etl.load.load_workouts import load_workouts
from etl.load.load_workout_exercises import load_workout_exercises
from etl.load.load_sets import load_sets


def run():
    print("=== ETL STARTED ===")

    load_exercises()
    load_workouts()
    load_workout_exercises()
    load_sets()

    print("=== ETL FINISHED ===")


if __name__ == "__main__":
    run()
```

This executed directly against:

> Amazon RDS (PostgreSQL)

## Idempotency Validation

The ETL pipeline was executed multiple times directly in Amazon RDS to validate replay safety and deterministic behavior. Across repeated executions, no duplicate exercises, workouts, or sets were created, and no referential integrity violations occurred. Row counts remained stable, confirming that the loading logic — including ON CONFLICT clauses and explicit existence checks — guarantees idempotent behavior and consistent state reconstruction.

This validates that the pipeline is replay-safe by design, meaning the same input dataset always produces the same relational state without unintended duplication or corruption.

---

## Production Characteristics

The ETL architecture was intentionally designed with production principles in mind. The modular structure, explicit foreign key mapping, deterministic execution order, and SQLAlchemy-based connectivity ensure that the pipeline can be easily extended into scheduled batch jobs, containerized workflows, or orchestrated environments such as Lambda or Airflow.

At this stage, validated CSV datasets were transformed into a fully normalized relational schema in Amazon RDS using PostgreSQL, idempotent SQL patterns, and controlled schema management. The system is now structurally stable, cloud-portable, and prepared for analytical aggregation in the Gold layer and subsequent LLM contract generation.

# Stage 5: Gold Layer

After loading the normalized relational model into **Amazon RDS (PostgreSQL)**, the next step was to build the **Gold Layer**.

This layer was designed specifically for:

* Weekly deterministic aggregation
* Safe week-over-week comparison
* Explicit metric semantics
* No hidden calculations
* LLM-ready consumption

All views were created directly inside the Amazon RDS instance under the `fitness` schema.

## Weekly Training Summary

This view aggregates training metrics at the weekly level.

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE OR REPLACE VIEW gold_weekly_fitness_summary AS
WITH training_summary AS (
    SELECT
        DATE_TRUNC('week', w.workout_date)::date AS week_start,
        COUNT(DISTINCT w.workout_id)             AS training_sessions,
        COUNT(s.set_id)                          AS total_sets,
        COUNT(*) FILTER (WHERE s.set_type = 'failure') AS failure_sets,
        ROUND(AVG(s.reps), 2)                    AS avg_reps_per_set
    FROM workouts w
    JOIN workout_exercises we
        ON w.workout_id = we.workout_id
    JOIN sets s
        ON we.workout_exercise_id = s.workout_exercise_id
    WHERE s.set_type <> 'warmup'
    GROUP BY week_start
)
SELECT
    d.week_start,

    -- Diet
    d.avg_calories_kcal,
    d.avg_carbs_g,
    d.avg_protein_g,
    d.avg_fat_g,
    d.avg_bodyweight_kg,
    d.cardio_weekly_min,
    d.logged_days,

    -- Training
    t.training_sessions,
    t.total_sets,
    t.failure_sets,
    t.avg_reps_per_set,

    -- Cycle
    c.cycle_name,
    c.cycle_type
FROM gold_weekly_diet_summary d
LEFT JOIN training_summary t
    ON d.week_start = t.week_start
LEFT JOIN cycles c
    ON d.cycle_id = c.cycle_id;
```
</details>

### Purpose

This creates a stable weekly anchor containing:

* Diet averages
* Training volume
* Failure intensity
* Active cycle

This becomes the **high-level weekly context layer**.

---

## Weekly Training Detail (Per Exercise)

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE OR REPLACE VIEW gold_weekly_training_detail AS
SELECT
    DATE_TRUNC('week', w.workout_date)::date AS week_start,

    e.exercise_name,
    mg.muscle_group_name,

    COUNT(s.set_id)                          AS total_sets,
    ROUND(AVG(s.weight_kg), 2)               AS avg_weight_kg,
    ROUND(AVG(s.reps), 2)                    AS avg_reps,
    MAX(s.weight_kg)                         AS max_weight_kg,

    COUNT(*) FILTER (WHERE s.set_type = 'failure') AS failure_sets
FROM workouts w
JOIN workout_exercises we
    ON w.workout_id = we.workout_id
JOIN exercises e
    ON we.exercise_id = e.exercise_id
JOIN exercise_muscle_map emm
    ON e.exercise_id = emm.exercise_id
JOIN muscle_groups mg
    ON emm.muscle_group_id = mg.muscle_group_id
JOIN sets s
    ON we.workout_exercise_id = s.workout_exercise_id
WHERE s.set_type <> 'warmup'
GROUP BY
    week_start,
    e.exercise_name,
    mg.muscle_group_name;
```
</details>

### Purpose

This view exposes:

* Volume by exercise
* Muscle group mapping
* Average and maximum loads
* Failure density

This enables:

* Muscle-specific analysis
* Volume allocation insights
* Performance segmentation

## Weekly Exercise Performance (Structured Metrics)

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE OR REPLACE VIEW gold_weekly_exercise_performance AS
SELECT
    DATE_TRUNC('week', w.workout_date)::date AS week_start,

    e.exercise_id,
    e.exercise_name,
    mg.muscle_group_name,

    COUNT(s.set_id) AS total_sets,

    COUNT(*) FILTER (
        WHERE s.set_type = 'failure'
    ) AS failure_sets,

    ROUND(AVG(s.reps), 2) AS avg_reps,

    ROUND(AVG(s.weight_kg), 2) AS avg_weight_kg,
    MAX(s.weight_kg)           AS max_weight_kg,

    SUM(s.reps) AS total_reps,

    COUNT(DISTINCT w.workout_id) AS sessions_count

FROM workouts w
JOIN workout_exercises we
    ON w.workout_id = we.workout_id
JOIN sets s
    ON we.workout_exercise_id = s.workout_exercise_id
JOIN exercises e
    ON we.exercise_id = e.exercise_id
JOIN exercise_muscle_map emm
    ON e.exercise_id = emm.exercise_id
JOIN muscle_groups mg
    ON emm.muscle_group_id = mg.muscle_group_id

WHERE s.set_type <> 'warmup'

GROUP BY
    week_start,
    e.exercise_id,
    e.exercise_name,
    mg.muscle_group_name;
```
</details>

### Purpose

This is the **LLM-facing metric layer**.

It produces:

* Explicit totals
* Explicit averages
* Explicit max loads
* Explicit session count

No inferred metrics.
No derived semantics.
Only controlled aggregations.

## Week-over-Week Progression

This view introduces temporal comparison using window functions.

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE OR REPLACE VIEW gold_weekly_exercise_progression AS
SELECT
    week_start,
    exercise_id,
    exercise_name,
    muscle_group_name,

    -- Current week metrics
    total_sets,
    total_reps,
    avg_reps,
    avg_weight_kg,
    max_weight_kg,
    failure_sets,
    sessions_count,

    -- Previous week metrics
    LAG(total_sets)      OVER w AS prev_total_sets,
    LAG(total_reps)      OVER w AS prev_total_reps,
    LAG(avg_reps)        OVER w AS prev_avg_reps,
    LAG(avg_weight_kg)   OVER w AS prev_avg_weight_kg,
    LAG(max_weight_kg)   OVER w AS prev_max_weight_kg,
    LAG(failure_sets)    OVER w AS prev_failure_sets,

    -- Deltas
    total_sets    - LAG(total_sets)    OVER w AS delta_total_sets,
    total_reps    - LAG(total_reps)    OVER w AS delta_total_reps,
    avg_reps      - LAG(avg_reps)      OVER w AS delta_avg_reps,
    avg_weight_kg - LAG(avg_weight_kg) OVER w AS delta_avg_weight_kg,
    max_weight_kg - LAG(max_weight_kg) OVER w AS delta_max_weight_kg,
    failure_sets  - LAG(failure_sets)  OVER w AS delta_failure_sets

FROM gold_weekly_exercise_performance
WINDOW w AS (
    PARTITION BY exercise_id
    ORDER BY week_start
);
```
</details>

### Purpose

This enables:

* Deterministic progression tracking
* Explicit deltas
* Safe LLM comparison

Instead of asking the LLM to compute differences, the database computes them deterministically.

## Final LLM Context View


This is the final structured analytical contract inside RDS.

<details>
<summary><strong>Show SQL</strong></summary>

```sql
CREATE OR REPLACE VIEW gold_llm_weekly_exercise_context AS
SELECT
    p.week_start,

    -- Exercise identity
    p.exercise_id,
    p.exercise_name,
    p.muscle_group_name,

    -- Training (current week)
    p.total_sets,
    p.total_reps,
    p.avg_reps,
    p.avg_weight_kg,
    p.max_weight_kg,
    p.failure_sets,
    p.sessions_count,

    -- Training deltas (vs previous week)
    p.delta_total_sets,
    p.delta_total_reps,
    p.delta_avg_reps,
    p.delta_avg_weight_kg,
    p.delta_max_weight_kg,
    p.delta_failure_sets,

    -- Diet context (weekly)
    d.avg_calories_kcal,
    d.avg_carbs_g,
    d.avg_protein_g,
    d.avg_fat_g,
    d.cardio_weekly_min,
    d.avg_bodyweight_kg,
    d.logged_days,

    -- Cycle context
    c.cycle_id,
    c.cycle_name,
    c.cycle_type

FROM gold_weekly_exercise_progression p
LEFT JOIN gold_weekly_diet_summary d
    ON p.week_start = d.week_start
LEFT JOIN cycles c
    ON d.cycle_id = c.cycle_id;
```
</details>

### Purpose

This is the final structured semantic layer consumed by the JSON extractor.

It merges:

* Exercise performance
* Progression metrics
* Diet context
* Cycle context

Into a single analytical surface.

## Gold Layer Summary

The Gold Layer was fully implemented inside Amazon RDS using SQL views.

# Stage 6: Canonical JSON (LLM)

After building the normalized relational model and the Gold analytical layer, the final step was to generate a canonical, deterministic JSON contract designed specifically for language model consumption.

The goal was not to simply serialize database rows, but to:

* Eliminate relational complexity
* Remove ambiguity
* Provide explicit deltas (no inference required)
* Deliver a schema-stable, versioned payload

This JSON acts as the single source of truth for LLM analysis.

## Design Principles

The canonical contract was designed under the following constraints:

1. **One file per week**
2. Fully deterministic structure
3. Explicit weekly progression deltas
4. No derived metrics inside the LLM
5. No missing semantic context
6. Schema versioning support

## Schema Definition (Versioned Contract)

The schema is explicitly defined and version-controlled:


<details>
<summary><strong>Show Code</strong></summary>

```json
{
  "week_start": "YYYY-MM-DD",
  "cycle": {
    "cycle_id": "int",
    "cycle_name": "string",
    "cycle_type": "cutting | bulking | maintenance | reverse"
  },
  "diet": {
    "avg_calories_kcal": "number",
    "avg_carbs_g": "number",
    "avg_protein_g": "number",
    "avg_fat_g": "number",
    "cardio_weekly_min": "number",
    "avg_bodyweight_kg": "number",
    "logged_days": "int"
  },
  "training_overview": {
    "training_sessions": "int",
    "total_sets": "int",
    "failure_sets": "int",
    "avg_reps_per_set": "number"
  },
  "exercises": [
    {
      "exercise_id": "int",
      "exercise_name": "string",
      "muscle_group": "string",
      "current_week": { ... },
      "delta_vs_last_week": { ... }
    }
  ]
}
```
</details>

## Payload Builder Implementation

The JSON is generated from Gold views using a dedicated extraction script.

```{python}
#| eval: false
import json
import os
from datetime import date
from decimal import Decimal

from sqlalchemy import text
from etl.utils.db import get_engine

# ===============================
# CONFIG
# ===============================

OUTPUT_DIR = "llm/payloads"
SCHEMA_VERSION = "v1"


# ===============================
# UTILS
# ===============================

def json_safe(value):
    """
    Converte tipos não serializáveis (Decimal, date, datetime)
    para formatos compatíveis com JSON.
    """
    if isinstance(value, Decimal):
        return float(value)
    if hasattr(value, "isoformat"):
        return value.isoformat()
    return value


# ===============================
# FETCH FUNCTIONS
# ===============================

def fetch_weekly_summary(conn, week_start):
    sql = text("""
        SELECT *
        FROM gold_weekly_fitness_summary
        WHERE week_start = :week_start
    """)
    row = conn.execute(
        sql,
        {"week_start": week_start}
    ).mappings().fetchone()

    return dict(row) if row else None


def fetch_exercise_context(conn, week_start):
    sql = text("""
        SELECT *
        FROM gold_llm_weekly_exercise_context_v1
        WHERE week_start = :week_start
        ORDER BY exercise_name
    """)

    rows = conn.execute(
        sql,
        {"week_start": week_start}
    ).mappings().fetchall()

    return [dict(r) for r in rows]


# ===============================
# PAYLOAD BUILDER
# ===============================

def build_payload(week_start: str):
    engine = get_engine()

    with engine.begin() as conn:
        weekly = fetch_weekly_summary(conn, week_start)
        if not weekly:
            raise RuntimeError(
                f"No weekly summary found for week_start={week_start}"
            )

        exercises_raw = fetch_exercise_context(conn, week_start)
        if not exercises_raw:
            raise RuntimeError(
                f"No exercise context found for week_start={week_start}"
            )

    # -------------------------------
    # Exercises
    # -------------------------------
    exercises = []

    for r in exercises_raw:
        exercises.append({
            "exercise_id": r["exercise_id"],
            "exercise_name": r["exercise_name"],
            "muscle_group": r["muscle_group_name"],

            "current_week": {
                "total_sets": r["current_total_sets"],
                "total_reps": r["current_total_reps"],
                "avg_reps": json_safe(r["current_avg_reps"]),
                "avg_weight_kg": json_safe(r["current_avg_weight_kg"]),
                "max_weight_kg": json_safe(r["current_max_weight_kg"]),
                "failure_sets": r["current_failure_sets"],
                "sessions_count": r["current_sessions"]
            },

            "delta_vs_last_week": {
                "total_sets": r["delta_sets"],
                "total_reps": r["delta_reps"],
                "avg_reps": json_safe(r["delta_avg_reps"]),
                "avg_weight_kg": json_safe(r["delta_avg_weight_kg"]),
                "max_weight_kg": json_safe(r["delta_max_weight_kg"]),
                "failure_sets": r["delta_failure_sets"]
            }
        })

    # -------------------------------
    # Final payload
    # -------------------------------
    payload = {
        "schema_version": SCHEMA_VERSION,
        "generated_at": date.today().isoformat(),
        "week_start": weekly["week_start"].isoformat(),

        "cycle": {
            "cycle_name": weekly["cycle_name"],
            "cycle_type": weekly["cycle_type"]
        },

        "diet": {
            "avg_calories_kcal": json_safe(weekly["avg_calories_kcal"]),
            "avg_carbs_g": json_safe(weekly["avg_carbs_g"]),
            "avg_protein_g": json_safe(weekly["avg_protein_g"]),
            "avg_fat_g": json_safe(weekly["avg_fat_g"]),
            "cardio_weekly_min": weekly["cardio_weekly_min"],
            "avg_bodyweight_kg": json_safe(weekly["avg_bodyweight_kg"]),
            "logged_days": weekly["logged_days"]
        },

        "training_overview": {
            "training_sessions": weekly["training_sessions"],
            "total_sets": weekly["total_sets"],
            "failure_sets": weekly["failure_sets"],
            "avg_reps_per_set": json_safe(weekly["avg_reps_per_set"])
        },

        "exercises": exercises
    }

    return payload


# ===============================
# SAVE
# ===============================

def save_payload(payload):
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    fname = f"weekly_fitness_context_{payload['week_start']}.json"
    path = os.path.join(OUTPUT_DIR, fname)

    with open(path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    return path


# ===============================
# MAIN
# ===============================

if __name__ == "__main__":
    # Use uma semana que você sabe que existe no banco
    WEEK_START = "2026-01-05"

    payload = build_payload(WEEK_START)
    path = save_payload(payload)

    print(f"[OK] Weekly context generated: {path}")
```


## Why This Structure Is LLM-Optimal

The JSON contract was intentionally designed to eliminate the most common failure modes observed when large language models consume structured data. Instead of requiring the model to infer joins, compute temporal comparisons, or aggregate metrics on the fly, all calculations are performed upstream in the Gold layer. The LLM receives a fully contextualized, semantically complete representation of the week.

There are no implicit relationships left to interpret, no derived metrics to calculate, and no schema ambiguity. Weekly deltas are explicitly provided, progression is already computed, and all nutritional and training context is embedded within the same document. As a result, the model operates purely as an analytical reasoning layer rather than a data transformation engine. This dramatically increases reliability, interpretability, and consistency of responses.

## Deterministic Properties

For any given week_start, the system always produces the exact same output. The canonical JSON is generated from stable Gold views, using a fixed schema definition, explicit ordering rules, and deterministic delta calculations based on window functions. There is no stochastic logic in the data layer, no dynamic field generation, and no schema mutation.

Because the transformation pipeline is idempotent and the Gold layer is reproducible, the JSON payload becomes a replay-safe artifact. Running the extraction process multiple times for the same week will always result in an identical file. This deterministic behavior is critical for safe LLM interaction, auditing, version control, and long-term system stability.

## Architectural Role

The JSON contract represents the formal boundary between structured analytical data and generative AI systems. Upstream of this boundary, the system is strictly relational, normalized, and governed by SQL logic inside Amazon RDS. Downstream of this boundary, the system becomes model-agnostic and cloud-portable.

Because the contract is independent of any specific model provider, it can be stored in S3, consumed by Amazon Bedrock, OpenAI models, or any future inference endpoint without requiring changes to the data layer. This separation of concerns ensures that the analytical infrastructure remains stable even as the AI layer evolves.

# Stage 7 — AWS Execution

After the relational modeling, Gold-layer aggregation, and canonical JSON generation were completed, the final stage of the project was executed entirely within the AWS ecosystem.

The weekly payload generated from Amazon RDS was uploaded to Amazon S3 and consumed directly through Amazon Bedrock. The LLM interaction occurred inside the AWS environment, ensuring that the structured data layer, storage layer, and inference layer remained cloud-native and internally integrated.

The inference process followed this flow:

Amazon RDS (Gold Views)
→ Canonical JSON Extraction
→ S3 Upload
→ Amazon Bedrock (LLM Inference)
→ Generated Weekly Performance Report

The prompt was manually defined inside the Bedrock playground, and the JSON payload was provided as structured context. The model was instructed to generate a deterministic weekly analytical report, focusing on progression, regression, diet alignment, fatigue signals, and performance insights — without inventing metrics or extrapolating beyond the provided contract.

The entire execution was recorded to demonstrate real AWS operation, including payload submission and model response generation.

---

## AWS Run-Once Execution (Bedrock Inference)

To demonstrate full cloud integration, the final analytical stage of the project was executed entirely inside Amazon Web Services.

This execution followed a run-once model, intentionally designed to validate the architecture while maintaining strict cost control. The objective was not continuous deployment, but architectural proof of integration between:

* Structured relational data (Amazon RDS)
* Canonical analytical artifact (JSON)
* Generative AI inference (Amazon Bedrock)

## S3 Bucket Creation and Canonical Payload Upload

The first step of the cloud execution layer was the creation of an Amazon S3 bucket to store the canonical JSON contract generated from the Gold layer.

The recorded demonstration includes:

* Creation of a dedicated S3 bucket
* Proper configuration and region selection
* Upload of the generated weekly JSON payload
* Verification of the object structure inside AWS

This step establishes S3 as the formal storage boundary between deterministic data engineering outputs and AI consumption layers.

By uploading the JSON to S3, the architecture aligns with production-grade patterns where:

* Analytical artifacts are versioned
* Outputs are cloud-stored
* Downstream services can consume structured data
* The data platform remains decoupled from inference engines

The S3 object represents the finalized analytical state of a given `week_start`.

{{< video Bucket_S3_creation.mp4 >}}

## Amazon Bedrock Prompt Execution and LLM Report Generation

After the canonical JSON was available in AWS, the next step was inference execution inside Amazon Bedrock.

In the recorded demonstration:

* The Amazon Bedrock Playground was opened
* A Claude model was selected
* The canonical JSON payload was inserted as structured context
* A carefully engineered analytical prompt was defined
* The model generated a weekly performance report
* The result was reviewed and validated against the structured input

The prompt was designed to:

* Constrain reasoning strictly to provided data
* Avoid external assumptions
* Focus on progression and regression signals
* Correlate diet and training metrics
* Produce structured analytical insights rather than narrative speculation

The model operates purely as an analytical reasoning layer, not as a data transformation layer.

This confirms that the system successfully transitions from:

Relational Modeling (RDS)
→ Analytical Aggregation (Gold Layer)
→ Deterministic JSON Contract
→ Cloud Storage (S3)
→ Generative Analysis (Bedrock)

The execution was run-once to minimize cloud costs, but the architecture fully supports future automation through event-driven or scheduled orchestration.

{{< video Claude.mp4 >}}

---

# Project Conclusion

This project demonstrates the full construction of an end-to-end analytical data platform, starting from raw operational CSV exports and evolving into a cloud-executed, AI-ready reporting system. Every stage was deliberately engineered with production principles in mind: explicit data auditing before ingestion, normalized relational modeling in Amazon RDS, idempotent ETL design, deterministic Gold-layer aggregations, and a strictly defined canonical JSON contract. Rather than relying on the LLM to interpret raw or semi-structured data, the architecture ensures that all business logic, aggregation rules, and temporal comparisons are resolved upstream — preserving analytical correctness and reproducibility.

By executing the final stage inside AWS using Amazon S3 and Amazon Bedrock, the system transitions from a traditional data platform into a structured generative analytics engine. The run-once deployment validates real cloud integration while maintaining cost efficiency, proving that the architecture is portable, reproducible, and ready for automation. What began as personal training and nutrition logs was transformed into a deterministic, model-consistent, and LLM-optimized analytical system — built with the rigor expected in professional Data Engineering and Analytics Engineering environments.

---

## GitHub Repository

Access all code, datasets, notebooks, and files for this project:

[Click here to access](https://github.com/FerreiraGabrielw/fitness-data-api-llm){target="_blank" rel="noopener noreferrer"}

---